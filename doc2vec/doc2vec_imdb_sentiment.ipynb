{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于gensim的doc2vec模型执行情感分类任务\n",
    "*数据集:* IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python27\\envs\\py36\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import gensim\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec,TaggedDocument, LabeledSentence\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "LabeledSentence = gensim.models.doc2vec.LabeledSentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "##读取并预处理数据\n",
    "def get_dataset():\n",
    "    #读取数据\n",
    "    pos_dir = \"D:/sourcecode/ai-dataset/aclImdb/train/pos\"\n",
    "    neg_dir = \"D:/sourcecode/ai-dataset/aclImdb/train/neg\"\n",
    "    unsup_dir = \"D:/sourcecode/ai-dataset/aclImdb/train/unsup\"\n",
    "\n",
    "    #对英文做简单的数据清洗预处理，中文根据需要进行修改\n",
    "    def cleanText(corpus):\n",
    "        punctuation = \"\"\".,?!:;(){}[]\"\"\"\n",
    "        corpus = [z.lower().replace('\\n','') for z in corpus]\n",
    "        corpus = [z.replace('<br />', ' ') for z in corpus]\n",
    "\n",
    "        #treat punctuation as individual words\n",
    "        for c in punctuation:\n",
    "            corpus = [z.replace(c, ' %s '%c) for z in corpus]\n",
    "        corpus = [z.split() for z in corpus]\n",
    "        return corpus\n",
    "\n",
    "    def read_dataset(rootdir):\n",
    "        docs = []\n",
    "        files = glob.glob(os.path.join(rootdir, '*.txt'))\n",
    "        for fn in files[:500]:\n",
    "            with open(fn, 'r', encoding='utf-8') as fin:\n",
    "                doc = fin.read()\n",
    "                justfn = os.path.split(fn)[-1].split('.')[0]\n",
    "                doc_id, score = justfn.split('_')\n",
    "                docs.append(TaggedDocument(simple_preprocess(doc), doc_id))\n",
    "        return docs\n",
    "    pos_reviews = read_dataset(pos_dir)\n",
    "    neg_reviews = read_dataset(neg_dir)\n",
    "    unsup_reviews = read_dataset(unsup_dir)\n",
    "    #使用1表示正面情感，0为负面\n",
    "    X = pos_reviews + neg_reviews\n",
    "    y = np.concatenate((np.ones(len(pos_reviews)), np.zeros(len(neg_reviews))))\n",
    "    #将数据分割为训练与测试集\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    del pos_reviews, neg_reviews\n",
    "    return x_train,x_test,unsup_reviews,y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "##读取向量\n",
    "def getVecs(model, corpus, size):\n",
    "    vecs = [np.array(model.docvecs[z.tags[0]]).reshape((1, size)) for z in corpus]\n",
    "    return np.concatenate(vecs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "##对数据进行训练\n",
    "def train(x_train,x_test,unsup_reviews,size = 100,epoch_num=10):\n",
    "    \n",
    "    #使用所有的数据建立词典\n",
    "    all_reviews = x_train + x_test + unsup_reviews\n",
    "    all_train_reviews = x_train + unsup_reviews\n",
    "\n",
    "    #实例DM和DBOW模型\n",
    "    model_dm = gensim.models.Doc2Vec(min_count=1, window=10, vector_size=size, sample=1e-3, negative=5, workers=8)\n",
    "    model_dbow = gensim.models.Doc2Vec(min_count=1, window=10, vector_size=size, sample=1e-3, negative=5, dm=0, workers=8)\n",
    "\n",
    "    model_dm.build_vocab(all_reviews)\n",
    "    model_dbow.build_vocab(all_reviews)\n",
    "\n",
    "    #进行多次重复训练，每一次都需要对训练数据重新打乱，以提高精度\n",
    "    model_dm.train(all_train_reviews,total_examples=model_dm.corpus_count,epochs=epoch_num)\n",
    "    model_dbow.train(all_train_reviews,total_examples=model_dbow.corpus_count,epochs=epoch_num)\n",
    "\n",
    "    #训练测试数据集\n",
    "    model_dm.train(x_test,total_examples=model_dm.corpus_count,epochs=epoch_num)\n",
    "    model_dbow.train(x_test,total_examples=model_dbow.corpus_count,epochs=epoch_num)\n",
    "\n",
    "    return model_dm,model_dbow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "##将训练完成的数据转换为vectors\n",
    "def get_vectors(model_dm,model_dbow):\n",
    "\n",
    "    #获取训练数据集的文档向量\n",
    "    train_vecs_dm = getVecs(model_dm, x_train, size)\n",
    "    train_vecs_dbow = getVecs(model_dbow, x_train, size)\n",
    "    train_vecs = np.hstack((train_vecs_dm, train_vecs_dbow))\n",
    "    #获取测试数据集的文档向量\n",
    "    test_vecs_dm = getVecs(model_dm, x_test, size)\n",
    "    test_vecs_dbow = getVecs(model_dbow, x_test, size)\n",
    "    test_vecs = np.hstack((test_vecs_dm, test_vecs_dbow))\n",
    "\n",
    "    return train_vecs,test_vecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "##使用分类器对文本向量进行分类训练\n",
    "def Classifier(train_vecs,y_train,test_vecs, y_test):\n",
    "    #使用sklearn的SGD分类器\n",
    "    from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "    lr = SGDClassifier(loss='log', penalty='l1')\n",
    "    lr.fit(train_vecs, y_train)\n",
    "\n",
    "    print('Test Accuracy: %.2f'%lr.score(test_vecs, y_test))\n",
    "\n",
    "    return lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "##绘出ROC曲线，并计算AUC\n",
    "def ROC_curve(lr,y_test):\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    pred_probas = lr.predict_proba(test_vecs)[:,1]\n",
    "\n",
    "    fpr,tpr,_ = roc_curve(y_test, pred_probas)\n",
    "    roc_auc = auc(fpr,tpr)\n",
    "    plt.plot(fpr,tpr,label='area = %.2f' %roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "##运行模块\n",
    "if __name__ == \"__main__\":\n",
    "    #设置向量维度和训练次数\n",
    "    size, epoch_num = 100, 40\n",
    "    #获取训练与测试数据及其类别标注\n",
    "    x_train,x_test,unsup_reviews,y_train, y_test = get_dataset()\n",
    "    #对数据进行训练，获得模型\n",
    "    model_dm,model_dbow = train(x_train,x_test,unsup_reviews,size,epoch_num)\n",
    "    #从模型中抽取文档相应的向量\n",
    "    train_vecs,test_vecs = get_vectors(model_dm,model_dbow)\n",
    "    #使用文章所转换的向量进行情感正负分类训练\n",
    "    lr=Classifier(train_vecs,y_train,test_vecs, y_test)\n",
    "    #画出ROC曲线\n",
    "    ROC_curve(lr,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
